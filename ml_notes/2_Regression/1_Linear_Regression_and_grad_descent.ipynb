{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b6a969",
   "metadata": {},
   "source": [
    "## Gradient Descent for Linear Regression\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b2d67c",
   "metadata": {},
   "source": [
    "## Gradient Descent with Single Variable Linear Regression\n",
    "\n",
    "### Overview\n",
    "This implementation demonstrates **gradient descent optimization** for a simple linear regression model with a single input variable.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**What is Gradient Descent?**\n",
    "Gradient descent is an iterative optimization algorithm that finds the optimal values of model parameters (intercept $b$ and slope $w$) by minimizing the error function.\n",
    "\n",
    "**How It Works:**\n",
    "- We use **numerical differentiation** (finite difference method) instead of analytical backpropagation\n",
    "- We estimate the slope (gradient) of the error function by slightly changing each parameter by a small amount ($\\delta$)\n",
    "- We take steps in the direction opposite to the gradient to reduce error\n",
    "\n",
    "### The Update Rule\n",
    "\n",
    "For each iteration, we update the parameters as:\n",
    "\n",
    "$$w_{\\text{next}} = w - \\text{learning\\_rate} \\times \\frac{dE}{dw}$$\n",
    "\n",
    "$$b_{\\text{next}} = b - \\text{learning\\_rate} \\times \\frac{dE}{db}$$\n",
    "\n",
    "Where:\n",
    "- $w$ = slope (weight) of the line\n",
    "- $b$ = intercept (bias) of the line\n",
    "- $\\frac{dE}{dw}$ = gradient (slope) of error with respect to $w$\n",
    "- $\\frac{dE}{db}$ = gradient (slope) of error with respect to $b$\n",
    "- learning_rate = controls the size of each step (typically a small value like 0.01)\n",
    "\n",
    "### Method Used\n",
    "We approximate the gradient by **numerical differentiation**:\n",
    "$$\\frac{dE}{dw} \\approx \\frac{E(w + \\delta) - E(w)}{\\delta}$$\n",
    "\n",
    "This avoids the need for calculus-based backpropagation and works for any differentiable error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37670aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample Data Points\n",
    "\n",
    "\"\"\"\n",
    "Here we are working with single variables with below data points.\n",
    "Here are 3 sample data points with input (x) and output (y) values:\n",
    "| Input (x) | Output (y) |\n",
    "|-----------|-----------|\n",
    "| 5         | 8         |\n",
    "| 2         | 4         |\n",
    "| 3         | 6         |\n",
    "\"\"\"\n",
    "delta = 1e-5  # A small value for numerical differentiation\n",
    "def compute_error_for_line_given_points(b, w, inputs):\n",
    "    return np.sum((y_actual - (w * inputs + b)) ** 2)\n",
    "\n",
    "def differntiate_error_wrt_b(b_current, w_current, inputs):\n",
    "    \"\"\"Compute the derivative of the error with respect to b (intercept).\n",
    "    Here we are not using backpropagation. We are just finding the slope of the error function with respect to b by changing b by very small amount delta.\n",
    "    \"\"\"\n",
    "    E = compute_error_for_line_given_points(b_current, w_current, inputs)\n",
    "    b_delta = b_current + delta\n",
    "    return (compute_error_for_line_given_points(b_delta, w_current, inputs) - E) / delta\n",
    "\n",
    "def differntiate_error_wrt_w(b_current, w_current, inputs):\n",
    "    \"\"\"Compute the derivative of the error with respect to w (slope).\n",
    "    Here we are not using backpropagation. We are just finding the slope of the error function with respect to w by changing w by very small amount delta.\n",
    "    \"\"\"\n",
    "    E = compute_error_for_line_given_points(b_current, w_current, inputs)\n",
    "    w_delta = w_current + delta\n",
    "    return (compute_error_for_line_given_points(b_current, w_delta, inputs) - E) / delta\n",
    "\n",
    "def find_weights_using_gradient_descent(inputs, learning_rate=0.01, num_iterations=1000):\n",
    "    \"\"\"Find optimal weights (b and w) using gradient descent.\n",
    "    \"\"\"\n",
    "    b = 0.0  # Initial intercept\n",
    "    w = 0.0  # Initial slope\n",
    "    for i in range(num_iterations):\n",
    "        b_gradient = differntiate_error_wrt_b(b, w, inputs)\n",
    "        w_gradient = differntiate_error_wrt_w(b, w, inputs)\n",
    "        b -= learning_rate * b_gradient\n",
    "        w -= learning_rate * w_gradient\n",
    "        error = compute_error_for_line_given_points(b, w, inputs)\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Error at iteration {i}: {error} with b={b}, w={w}\")\n",
    "    return b, w\n",
    "\n",
    "x = np.array([5, 2, 3])\n",
    "w_actual = 3\n",
    "b_actual = 5\n",
    "y_actual = w_actual * x + b_actual\n",
    "print(f\"y_actual: {y_actual}\")\n",
    "b_optimal, w_optimal = find_weights_using_gradient_descent(x)\n",
    "print(f\"Optimal weights: b={b_optimal}, w={w_optimal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c40dc",
   "metadata": {},
   "source": [
    "## Gradient Descent with Two Variables\n",
    "\n",
    "### From 1 Variable to 2 Variables\n",
    "\n",
    "**Single Variable (Univariate):**\n",
    "- Linear equation: $y = w \\cdot x + b$\n",
    "- Input: One feature ($x$)\n",
    "- Parameters to optimize: slope ($w$) and intercept ($b$)\n",
    "- Error function: $E = \\sum (y_{\\text{actual}} - (w \\cdot x + b))^2$\n",
    "\n",
    "**Two Variables (Bivariate):**\n",
    "- Linear equation: $y = w_1 \\cdot x_1 + w_2 \\cdot x_2 + b$\n",
    "- Inputs: Two features ($x_1$, $x_2$)\n",
    "- Parameters to optimize: two weights ($w_1$, $w_2$) and intercept ($b$)\n",
    "- Error function: $E = \\sum (y_{\\text{actual}} - (w_1 \\cdot x_1 + w_2 \\cdot x_2 + b))^2$\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Aspect | 1 Variable | 2 Variables |\n",
    "|--------|-----------|------------|\n",
    "| **Input shape** | `(n,)` - 1D array | `(n, 2)` - 2D array with 2 columns |\n",
    "| **Weights** | 1 weight ($w$) | 2 weights ($w_1$, $w_2$) |\n",
    "| **Prediction** | `y = w*x + b` | `y = w1*x1 + w2*x2 + b` |\n",
    "| **Gradients** | $\\frac{dE}{dw}$, $\\frac{dE}{db}$ | $\\frac{dE}{dw_1}$, $\\frac{dE}{dw_2}$, $\\frac{dE}{db}$ |\n",
    "\n",
    "### Implementation Strategy\n",
    "\n",
    "The error computation remains **conceptually the same**, just with more input features:\n",
    "\n",
    "```python\n",
    "def compute_error_for_line_given_points(b, w1, w2, x1, x2):\n",
    "    predictions = w1*x1 + w2*x2 + b\n",
    "    return np.sum((y_actual - predictions) ** 2)\n",
    "```\n",
    "\n",
    "Or more elegantly using **matrix multiplication**:\n",
    "\n",
    "```python\n",
    "def compute_error_for_line_given_points(b, weights, inputs):\n",
    "    # inputs shape: (n, 2) where n is number of samples\n",
    "    # weights shape: (2,) containing [w1, w2]\n",
    "    predictions = np.dot(inputs, weights) + b\n",
    "    return np.sum((y_actual - predictions) ** 2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec3718",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample Data with 2 Input Variables\n",
    "\n",
    "\"\"\"\n",
    "Sample data with 2 input features (x1, x2) and 1 output (y):\n",
    "| x1 | x2 | y (actual) |\n",
    "|----|----|----|\n",
    "| 1  | 2  | 8  |\n",
    "| 2  | 3  | 13 |\n",
    "| 3  | 4  | 18 |\n",
    "| 4  | 5  | 23 |\n",
    "\n",
    "Relationship: y = 2*x1 + 3*x2 + 1 (w1=2, w2=3, b=1)\n",
    "\"\"\"\n",
    "\n",
    "delta = 1e-5\n",
    "\n",
    "def compute_error_for_line_given_points_2vars(b, w1, w2, x_data):\n",
    "    predictions = w1 * x_data[:, 0] + w2 * x_data[:, 1] + b\n",
    "    return np.sum((y_actual - predictions) ** 2)\n",
    "\n",
    "def differentiate_error_wrt_b_2vars(b_current, w1_current, w2_current, x_data):\n",
    "    \"\"\"Gradient with respect to b (intercept)\"\"\"\n",
    "    E = compute_error_for_line_given_points_2vars(b_current, w1_current, w2_current, x_data)\n",
    "    b_delta = b_current + delta\n",
    "    return (compute_error_for_line_given_points_2vars(b_delta, w1_current, w2_current, x_data) - E) / delta\n",
    "\n",
    "def differentiate_error_wrt_w1_2vars(b_current, w1_current, w2_current, x_data):\n",
    "    \"\"\"Gradient with respect to w1\"\"\"\n",
    "    E = compute_error_for_line_given_points_2vars(b_current, w1_current, w2_current, x_data)\n",
    "    w1_delta = w1_current + delta\n",
    "    return (compute_error_for_line_given_points_2vars(b_current, w1_delta, w2_current, x_data) - E) / delta\n",
    "\n",
    "def differentiate_error_wrt_w2_2vars(b_current, w1_current, w2_current, x_data):\n",
    "    \"\"\"Gradient with respect to w2\"\"\"\n",
    "    E = compute_error_for_line_given_points_2vars(b_current, w1_current, w2_current, x_data)\n",
    "    w2_delta = w2_current + delta\n",
    "    return (compute_error_for_line_given_points_2vars(b_current, w1_current, w2_delta, x_data) - E) / delta\n",
    "\n",
    "def find_weights_using_gradient_descent_2vars(x_data, learning_rate=0.0001, num_iterations=1500):\n",
    "    b = 0.0\n",
    "    w1 = 0.0\n",
    "    w2 = 0.0\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        b_gradient = differentiate_error_wrt_b_2vars(b, w1, w2, x_data)\n",
    "        w1_gradient = differentiate_error_wrt_w1_2vars(b, w1, w2, x_data)\n",
    "        w2_gradient = differentiate_error_wrt_w2_2vars(b, w1, w2, x_data)\n",
    "        \n",
    "        b -= learning_rate * b_gradient\n",
    "        w1 -= learning_rate * w1_gradient\n",
    "        w2 -= learning_rate * w2_gradient\n",
    "        \n",
    "        error = compute_error_for_line_given_points_2vars(b, w1, w2, x_data)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Iteration {i}: Error={error:.4f}, b={b:.4f}, w1={w1:.4f}, w2={w2:.4f}\")\n",
    "    \n",
    "    return b, w1, w2\n",
    "\n",
    "# Create sample data\n",
    "x1 = np.array([1, 2, 3, 4])\n",
    "x2 = np.array([2, 3, 4, 5])\n",
    "x_data = np.column_stack((x1, x2))  # Shape: (4, 2)\n",
    "\n",
    "# Actual relationship: y = 2*x1 + 3*x2 + 1\n",
    "w1_actual = 5\n",
    "w2_actual = 12\n",
    "b_actual = 8\n",
    "y_actual = w1_actual * x1 + w2_actual * x2 + b_actual\n",
    "\n",
    "print(\"Sample Data:\")\n",
    "print(f\"x1: {x1}\")\n",
    "print(f\"x2: {x2}\")\n",
    "print(f\"y_actual: {y_actual}\")\n",
    "print()\n",
    "\n",
    "# Run gradient descent\n",
    "print(\"Running Gradient Descent:\")\n",
    "b_optimal, w1_optimal, w2_optimal = find_weights_using_gradient_descent_2vars(x_data, learning_rate=0.001, num_iterations=500)\n",
    "print()\n",
    "print(f\"Optimal parameters: b={b_optimal:.4f}, w1={w1_optimal:.4f}, w2={w2_optimal:.4f}\")\n",
    "print(f\"Actual parameters:  b={b_actual}, w1={w1_actual}, w2={w2_actual}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
