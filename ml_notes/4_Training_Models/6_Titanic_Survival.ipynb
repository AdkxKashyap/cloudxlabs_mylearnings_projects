{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "100c9702",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction â€” Project Summary\n",
    "\n",
    "## What is This Project About?\n",
    "\n",
    "We're building a **classification model** to predict whether a passenger on the Titanic survived or not, based on their characteristics:\n",
    "- **Features:** Passenger class, Sex, Age, Number of siblings/spouses, Number of parents/children, Fare paid, Port of embarkation\n",
    "- **Target:** Survived (1) or Did Not Survive (0)\n",
    "\n",
    "This is a **binary classification problem** (two outcomes: yes or no).\n",
    "\n",
    "---\n",
    "\n",
    "## Data Pipeline\n",
    "\n",
    "```\n",
    "Raw Data\n",
    "   â†“\n",
    "[Data Preprocessing]\n",
    "   â”œâ”€ Numerical Features (Age, Fare, etc.) â†’ Median Imputation\n",
    "   â””â”€ Categorical Features (Sex, Embarked, etc.) â†’ Mode Imputation + One-Hot Encoding\n",
    "   â†“\n",
    "[Combined Features]\n",
    "   â†“\n",
    "[Train Models & Evaluate]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Model 1: Support Vector Classifier (SVC)\n",
    "\n",
    "### How SVC Works\n",
    "\n",
    "SVC finds the **best decision boundary** that separates \"Survived\" from \"Did Not Survive\" passengers.\n",
    "\n",
    "**Think of it like drawing a line on a map:**\n",
    "- One side of the line = Survived (1)\n",
    "- Other side of the line = Did Not Survive (0)\n",
    "- The goal: Maximize the distance between the line and the data points\n",
    "\n",
    "### RBF Kernel â€” Handling Complex Patterns\n",
    "\n",
    "```python\n",
    "SVC(gamma=\"auto\", random_state=42)\n",
    "```\n",
    "\n",
    "**Kernel:** Determines how SVC draws the decision boundary\n",
    "- **Linear Kernel:** Straight line (for linearly separable data)\n",
    "- **RBF (Radial Basis Function) Kernel:** Curved, complex boundaries (for non-linear patterns)\n",
    "\n",
    "We use **RBF** because survival data is complex â€” a straight line won't separate \"survived\" from \"did not survive\" well enough.\n",
    "\n",
    "**Visual example:**\n",
    "\n",
    "```\n",
    "Linear Kernel (simple)     RBF Kernel (complex)\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”     â•±â•²â•±â•²â•±â•²â•±â•²â•±â•²â•±â•²\n",
    "â”‚  â—‹ â—‹ â—‹                     â”‚ â—‹ â•² â—‹ â•² â—‹\n",
    "â”‚ â”€ â”€ â”€ â”€ (line)            â”œâ”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€ (wavy curve)\n",
    "â”‚  Ã— Ã— Ã—                     â”‚ Ã— â•± Ã— â•± Ã—\n",
    "```\n",
    "\n",
    "### Gamma Parameter â€” How \"Local\" the Decision is\n",
    "\n",
    "**Gamma = \"auto\"** means sklearn automatically calculates it as `1 / (n_features)`\n",
    "\n",
    "**Gamma controls:** How far the influence of each training example reaches\n",
    "- **Low Gamma (â‰¤ 0.01):** Each example has **far-reaching influence** â†’ Simple, smooth boundaries â†’ Might underfit\n",
    "- **High Gamma (â‰¥ 0.1):** Each example has **local influence only** â†’ Complex, wiggly boundaries â†’ Might overfit\n",
    "- **\"auto\":** Usually gives a good balance\n",
    "\n",
    "**Visual example:**\n",
    "\n",
    "```\n",
    "Low Gamma                  High Gamma\n",
    "(smooth boundary)          (complex boundary)\n",
    "    â•±â”€â”€â”€â”€â”€â•²                â•±â•²â•±â•²â•±â•²â•±â•²\n",
    "   â”‚ â—‹ â—‹ â—‹ â”‚              â”‚â—‹â•² â—‹â•±â—‹â•²\n",
    "â”€â”€â”€â”¤   â”Œâ”€â”€â” â”œâ”€â”€â”€           â”œâ”€â—â”€â—â”€â—â”€â”¤\n",
    "   â”‚ Ã— Ã— Ã— â”‚              â”‚Ã—â•± Ã—â•²Ã—â•±\n",
    "    â•²â”€â”€â”€â”€â”€â•±                â•²â•±â•²â•±â•²â•±â•²â•±\n",
    "```\n",
    "\n",
    "### SVC Results\n",
    "\n",
    "```python\n",
    "svm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\n",
    "# Mean accuracy: ~78-80%\n",
    "```\n",
    "\n",
    "âœ… **Good start**, but can we do better?\n",
    "\n",
    "---\n",
    "\n",
    "## Model 2: Random Forest Classifier\n",
    "\n",
    "### How Random Forest Works\n",
    "\n",
    "**Random Forest = \"Forest\" of Decision Trees**\n",
    "\n",
    "Instead of one tree, we grow **100 decision trees** (n_estimators=100), each trained on a **random subset** of data:\n",
    "\n",
    "```python\n",
    "RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "```\n",
    "\n",
    "**How it makes predictions:**\n",
    "1. Each tree makes its own prediction (Survived or Did Not Survive)\n",
    "2. **Vote:** All 100 trees vote\n",
    "3. **Winner:** Prediction with the most votes wins\n",
    "\n",
    "**Example with 3 trees:**\n",
    "```\n",
    "Tree 1 says: \"Survived\"  âœ“\n",
    "Tree 2 says: \"Survived\"  âœ“\n",
    "Tree 3 says: \"Did Not\"   âœ—\n",
    "            â”€â”€â”€â”€â”€â”€â”€\n",
    "Result: \"Survived\" (2 out of 3 votes)\n",
    "```\n",
    "\n",
    "### Why Random Forest is Better for This Dataset\n",
    "\n",
    "| Aspect | SVC | Random Forest |\n",
    "|--------|-----|---------------|\n",
    "| **Handles non-linear patterns** | Good (with RBF kernel) | Excellent (naturally) |\n",
    "| **Handles mixed data types** | Needs preprocessing | Handles easily |\n",
    "| **Interactions between features** | Struggles | Naturally captures |\n",
    "| **Interpretability** | \"Black box\" | Can see feature importance |\n",
    "| **Robustness** | Sensitive to outliers | Less sensitive (voting) |\n",
    "| **Speed** | Fast training | Slower but worth it |\n",
    "\n",
    "### Random Forest Results\n",
    "\n",
    "```python\n",
    "forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\n",
    "# Mean accuracy: ~81-83%\n",
    "```\n",
    "\n",
    "**Random Forest performs better!** ğŸ‰\n",
    "\n",
    "**Why?** The Titanic dataset has complex relationships:\n",
    "- Women had higher survival rates\n",
    "- Higher passenger class â†’ Higher survival rates\n",
    "- Age mattered (children vs. adults)\n",
    "- Fare amount (wealth) mattered\n",
    "\n",
    "These **non-linear interactions** are exactly what Random Forest excels at.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison\n",
    "\n",
    "```\n",
    "Model Accuracy Comparison\n",
    "â”‚\n",
    "â”‚ 82% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€ Random Forest (WINNER)\n",
    "â”‚ 80% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SVC\n",
    "â”‚ 78% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    SVC        Random Forest\n",
    "```\n",
    "\n",
    "**Lesson:** Different models work better for different problems. **Always try multiple models!**\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **SVC with RBF kernel** is powerful but requires careful tuning (gamma, C parameters)\n",
    "2. **Random Forest** is more flexible and often works better with real-world, messy data\n",
    "3. **Cross-validation** gives us honest performance estimates (not overly optimistic)\n",
    "4. **Feature engineering** (preprocessing pipelines) is crucial for both models\n",
    "5. **Always compare models** before choosing one for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f03b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "# Download dataset: https://www.kaggle.com/c/titanic/data\n",
    "TITANIC_PATH = '../datasets/titanic'\n",
    "def load_titanic_data(filename, titanic_path=TITANIC_PATH):\n",
    "    csv_path = os.path.join(titanic_path, filename)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47618375",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data. Data is divided into train and test sets. We will use the train set to train our model and the test set to evaluate its performance.\n",
    "train_data = load_titanic_data(\"train.csv\")\n",
    "test_data = load_titanic_data(\"test.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50182f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data to understand its structure and identify any missing values or outliers. This will help us decide how to preprocess the data before training our model.\n",
    "train_data.info()\n",
    "# 3 attributes have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6610920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61653014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coount of female passengers\n",
    "train_data[\"Sex\"].value_counts()[\"female\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a67b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom transformer to select specific columns from the DataFrame. This is useful when we want to apply different preprocessing steps to different subsets of features.\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170dd1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Creating a pipeline for preprocessing the numerical attributes. This pipeline will first select the numerical attributes, then impute any missing values using the median strategy.\n",
    "num_pipeline = Pipeline([\n",
    "    (\"select_numeric\", DataFrameSelector([\"Age\", \"SibSp\", \"Parch\", \"Fare\"])),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ede1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e78136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom transformer to impute missing values in categorical attributes with the most frequent value. This is useful when we want to handle missing values in categorical features before training our model.\n",
    "#Suppose sex column has missing values, we can impute them with the most frequent value in that column.Like if most of the values in the sex column are male, then we can impute the missing values with male.\n",
    "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # Compute the most frequent value for each column and store it in an attribute called most_frequent_. This will be used later in the transform method to fill the missing values.\n",
    "        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n",
    "                                        index=X.columns)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.most_frequent_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_pipeline = Pipeline([\n",
    "        (\"select_cat\", DataFrameSelector([\"Pclass\", \"Sex\", \"Embarked\"])),\n",
    "        (\"imputer\", MostFrequentImputer()),\n",
    "        (\"cat_encoder\", OneHotEncoder()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ccd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the numerical and categorical pipelines into a single pipeline using FeatureUnion. This allows us to apply different preprocessing steps to different subsets of features and then combine the results into a single feature matrix that can be used to train our model.\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "preprocess_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa649cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the features and the target variable. The target variable is \"Survived\", which indicates whether a passenger survived or not. The features are all the other columns in the dataset.\n",
    "X_train = preprocess_pipeline.fit_transform(train_data)\n",
    "y_train = train_data[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b95dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train SVC Classifier\n",
    "from sklearn.svm import SVC\n",
    "svm_clf  = SVC(gamma=\"auto\", random_state=42)\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d3845",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = preprocess_pipeline.transform(test_data)\n",
    "y_pred = svm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c95c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9304d0",
   "metadata": {},
   "source": [
    "# Cross-Validation Score â€” What It Tells Us and How It's Computed\n",
    "\n",
    "## What the Score Tells Us\n",
    "\n",
    "The score represents **accuracy** (for classification tasks like Titanic survival prediction):\n",
    "- **Range:** 0 to 1 (or 0% to 100%)\n",
    "- **0.85 means:** The model correctly predicts survival for 85% of passengers\n",
    "- **Returns:** An array of scores from each fold\n",
    "\n",
    "---\n",
    "\n",
    "## How It's Computed (K-Fold Cross-Validation)\n",
    "\n",
    "With `cv=10`, the data is split into **10 folds** (10 parts):\n",
    "\n",
    "```\n",
    "Original Dataset\n",
    "â”‚\n",
    "â”œâ”€ Fold 1: Test on fold 1, Train on folds 2-10 â†’ Score 1\n",
    "â”œâ”€ Fold 2: Test on fold 2, Train on folds 1,3-10 â†’ Score 2\n",
    "â”œâ”€ Fold 3: Test on fold 3, Train on folds 1-2,4-10 â†’ Score 3\n",
    "...\n",
    "â””â”€ Fold 10: Test on fold 10, Train on folds 1-9 â†’ Score 10\n",
    "\n",
    "Result: [score1, score2, score3, ..., score10]\n",
    "```\n",
    "\n",
    "**Process for each fold:**\n",
    "1. **Train** the model on 9 folds (90% of data)\n",
    "2. **Test** the model on 1 fold (10% of data)\n",
    "3. **Record** the accuracy score\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use Cross-Validation Instead of Train/Test Split?\n",
    "\n",
    "| Approach | Benefit | Drawback |\n",
    "|---|---|---|\n",
    "| **Single Train/Test Split** | Simple, fast | High variance (depends on which examples in test set) |\n",
    "| **Cross-Validation (cv=10)** | Reliable, uses all data for training | Slower (trains 10 times) |\n",
    "\n",
    "**Cross-validation solves the variance problem:**\n",
    "- You get 10 scores â†’ You see the **range and average**\n",
    "- More reliable estimate of real-world performance\n",
    "\n",
    "---\n",
    "\n",
    "## Interpreting Cross-Validation Results\n",
    "\n",
    "```python\n",
    "svm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\n",
    "# Output: array([0.82, 0.85, 0.88, 0.80, 0.86, 0.84, 0.87, 0.83, 0.85, 0.81])\n",
    "\n",
    "print(f\"Mean: {svm_scores.mean():.3f}\")  # 0.841 (average accuracy)\n",
    "print(f\"Std:  {svm_scores.std():.3f}\")   # 0.024 (how much scores vary)\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- âœ… Your SVM model has ~84.1% average accuracy\n",
    "- âœ… The Â±2.4% variation is small â†’ model is stable\n",
    "- âœ… Not overfitting (scores are consistent across folds)\n",
    "\n",
    "---\n",
    "\n",
    "## Common cv Values\n",
    "\n",
    "| cv | Use Case |\n",
    "|---|---|\n",
    "| **5** | Standard, good balance of speed/reliability |\n",
    "| **10** | More thorough, slower |\n",
    "| **LeaveOneOut** | Maximum accuracy but very slow |\n",
    "| **StratifiedKFold** | Better for imbalanced classes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\n",
    "svm_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb76afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check using Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\n",
    "forest_scores.mean()\n",
    "#Random Forest Classifier performs better than SVC Classifier in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21fe68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_cloudxlabs (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
