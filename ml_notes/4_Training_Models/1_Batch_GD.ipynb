{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18fd6e41",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent — Quick Overview\n",
    "\n",
    "This notebook uses **batch gradient descent** to fit a linear model. Keep this short reference handy.\n",
    "\n",
    "What is Batch Gradient Descent?\n",
    "- We update the model parameters (`theta`) by computing the gradient of the loss over the *entire training set* each iteration.\n",
    "- Update rule (mean squared error loss):\n",
    "\n",
    "  theta := theta - eta * grad\n",
    "\n",
    "How the gradient is calculated (intuitively and mathematically):\n",
    "- Predictions: p = X · theta  \n",
    "  (X shape = (m, n), theta shape = (n, 1))\n",
    "- Error vector: e = p − y  \n",
    "  (y shape = (m, 1), so e is (m, 1))\n",
    "- Gradient of MSE w.r.t. theta:  \n",
    "  grad = (2/m) · X^T · e  \n",
    "  - X^T has shape (n, m), X^T·e → (n, 1) → one gradient value per parameter\n",
    "\n",
    "Why the shapes matter\n",
    "- Keep `theta` as (n, 1) and `y` as (m, 1) to avoid unintended broadcasting and to produce gradients shaped (n, 1).\n",
    "\n",
    "Practical tips\n",
    "- `eta` (learning rate) controls step size — too large diverges, too small is slow.\n",
    "- Feature scaling helps convergence (standardize inputs before training).\n",
    "- Initialize `theta` randomly or with zeros; set a random seed for reproducibility.\n",
    "- Monitor training: print the loss or parameter values every few iterations.\n",
    "\n",
    "This short note explains the code below that computes `error`, `calc_gradient`, and the loop that updates `theta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f99d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b516878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(X:np.ndarray, theta, y_actual):\n",
    "    return (X.dot(theta) - y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c874039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient(X:np.ndarray, theta, y_actual):\n",
    "    err = error(X, theta, y_actual)\n",
    "    m = len(y_actual)\n",
    "    grad = 2/m * X.T.dot(err)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "835e6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient descent\n",
    "def compute_weights(X, y_actual):\n",
    "    eta = 0.01\n",
    "    iterations = 5000\n",
    "    theta = np.random.randn(3,1) #Init theta values. These are initial random weights\n",
    "    for i in range(iterations):\n",
    "        grads = calc_gradient(X, theta, y_actual)\n",
    "        theta = theta - eta * grads\n",
    "        if(i % 50 == 0):\n",
    "            print(f\"Iteration {i}: theta={theta}, error={error(X, theta, y_actual)}\")\n",
    "    return theta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd9970f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-matrix: [[1.         0.37454012 0.95071431]\n",
      " [1.         0.73199394 0.59865848]\n",
      " [1.         0.15601864 0.15599452]]\n",
      "y_Actual: [[11.1244326 ]\n",
      " [12.5812379 ]\n",
      " [ 5.18414037]]\n",
      "Iteration 0: theta=[[ 1.73910529]\n",
      " [ 0.84768611]\n",
      " [-0.3614787 ]], error=[[ -9.41149783]\n",
      " [-10.4380338 ]\n",
      " [ -3.36916893]]\n",
      "Iteration 50: theta=[[5.58038103]\n",
      " [2.97515301]\n",
      " [2.44096351]], error=[[-2.10907848]\n",
      " [-3.36175938]\n",
      " [ 1.24119692]]\n",
      "Iteration 100: theta=[[6.18425716]\n",
      " [3.61956586]\n",
      " [3.19866108]], error=[[-0.54348997]\n",
      " [-1.83257486]\n",
      " [ 2.06381014]]\n",
      "Iteration 150: theta=[[6.12706445]\n",
      " [3.94404316]\n",
      " [3.51389262]], error=[[-0.17945777]\n",
      " [-1.46353612]\n",
      " [ 2.10641633]]\n",
      "Iteration 200: theta=[[5.94533037]\n",
      " [4.19192146]\n",
      " [3.7223351 ]], error=[[-0.07018223]\n",
      " [-1.33903892]\n",
      " [ 1.99587177]]\n",
      "Iteration 250: theta=[[5.75000425]\n",
      " [4.41455808]\n",
      " [3.89524747]], error=[[-0.01773175]\n",
      " [-1.26788093]\n",
      " [ 1.86225449]]\n",
      "Iteration 300: theta=[[5.56330417]\n",
      " [4.62321052]\n",
      " [4.04861288]], error=[[ 0.01952357]\n",
      " [-1.21003518]\n",
      " [ 1.73203225]]\n",
      "Iteration 350: theta=[[5.38900362]\n",
      " [4.82073307]\n",
      " [4.18678986]], error=[[ 0.05056997]\n",
      " [-1.15702961]\n",
      " [ 1.61010375]]\n",
      "Iteration 400: theta=[[5.2270932 ]\n",
      " [5.00819367]\n",
      " [4.31162841]], error=[[ 0.07755686]\n",
      " [-1.10698435]\n",
      " [ 1.49691481]]\n",
      "Iteration 450: theta=[[5.07683512]\n",
      " [5.18625703]\n",
      " [4.42437893]], error=[[ 0.1011842 ]\n",
      " [-1.05940206]\n",
      " [ 1.3920264 ]]\n",
      "Iteration 500: theta=[[4.93739587]\n",
      " [5.3554754 ]\n",
      " [4.52609663]], error=[[ 0.12182849]\n",
      " [-1.01408032]\n",
      " [ 1.29485577]]\n",
      "Iteration 550: theta=[[4.80797391]\n",
      " [5.51635119]\n",
      " [4.61773143]], error=[[ 0.13977948]\n",
      " [-0.97088423]\n",
      " [ 1.20482796]]\n",
      "Iteration 600: theta=[[4.6878223 ]\n",
      " [5.66935189]\n",
      " [4.7001519 ]], error=[[ 0.15529109]\n",
      " [-0.92969854]\n",
      " [ 1.12140445]]\n",
      "Iteration 650: theta=[[4.57624973]\n",
      " [5.81491488]\n",
      " [4.77415496]], error=[[ 0.16859346]\n",
      " [-0.89041733]\n",
      " [ 1.04408649]]\n",
      "Iteration 700: theta=[[4.47261731]\n",
      " [5.95345   ]\n",
      " [4.84047225]], error=[[ 0.1798968 ]\n",
      " [-0.85294147]\n",
      " [ 0.97241327]]\n",
      "Iteration 750: theta=[[4.37633478]\n",
      " [6.08534157]\n",
      " [4.89977551]], error=[[ 0.1893934 ]\n",
      " [-0.81717778]\n",
      " [ 0.90595926]]\n",
      "Iteration 800: theta=[[4.28685678]\n",
      " [6.21095011]\n",
      " [4.95268131]], error=[[ 0.19725915]\n",
      " [-0.78303857]\n",
      " [ 0.84433155]]\n",
      "Iteration 850: theta=[[4.20367943]\n",
      " [6.33061402]\n",
      " [4.99975557]], error=[[ 0.2036549 ]\n",
      " [-0.75044127]\n",
      " [ 0.78716732]]\n",
      "Iteration 900: theta=[[4.1263371 ]\n",
      " [6.44465103]\n",
      " [5.04151752]], error=[[ 0.20872769]\n",
      " [-0.71930805]\n",
      " [ 0.73413153]]\n",
      "Iteration 950: theta=[[4.05439951]\n",
      " [6.55335963]\n",
      " [5.07844345]], error=[[ 0.21261185]\n",
      " [-0.68956558]\n",
      " [ 0.68491476]]\n",
      "Iteration 1000: theta=[[3.98746899]\n",
      " [6.65702035]\n",
      " [5.11097021]], error=[[ 0.21543008]\n",
      " [-0.66114466]\n",
      " [ 0.63923124]]\n",
      "Iteration 1050: theta=[[3.92517797]\n",
      " [6.75589695]\n",
      " [5.13949829]], error=[[ 0.21729437]\n",
      " [-0.63398003]\n",
      " [ 0.59681703]]\n",
      "Iteration 1100: theta=[[3.86718666]\n",
      " [6.85023755]\n",
      " [5.1643948 ]], error=[[ 0.21830687]\n",
      " [-0.60801008]\n",
      " [ 0.55742834]]\n",
      "Iteration 1150: theta=[[3.81318095]\n",
      " [6.94027565]\n",
      " [5.18599614]], error=[[ 0.21856074]\n",
      " [-0.58317662]\n",
      " [ 0.52083994]]\n",
      "Iteration 1200: theta=[[3.76287041]\n",
      " [7.02623109]\n",
      " [5.20461046]], error=[[ 0.21814086]\n",
      " [-0.55942469]\n",
      " [ 0.48684378]]\n",
      "Iteration 1250: theta=[[3.71598648]\n",
      " [7.10831095]\n",
      " [5.22051994]], error=[[ 0.2171245 ]\n",
      " [-0.53670231]\n",
      " [ 0.45524762]]\n",
      "Iteration 1300: theta=[[3.67228079]\n",
      " [7.18671039]\n",
      " [5.23398291]], error=[[ 0.21558198]\n",
      " [-0.51496037]\n",
      " [ 0.42587386]]\n",
      "Iteration 1350: theta=[[3.63152364]\n",
      " [7.26161338]\n",
      " [5.24523576]], error=[[ 0.21357725]\n",
      " [-0.49415237]\n",
      " [ 0.39855836]]\n",
      "Iteration 1400: theta=[[3.59350253]\n",
      " [7.33319349]\n",
      " [5.25449471]], error=[[ 0.21116839]\n",
      " [-0.47423432]\n",
      " [ 0.37314943]]\n",
      "Iteration 1450: theta=[[3.5580209 ]\n",
      " [7.40161445]\n",
      " [5.26195748]], error=[[ 0.20840811]\n",
      " [-0.45516457]\n",
      " [ 0.34950689]]\n",
      "Iteration 1500: theta=[[3.52489684]\n",
      " [7.46703089]\n",
      " [5.26780477]], error=[[ 0.20534424]\n",
      " [-0.43690366]\n",
      " [ 0.32750116]]\n",
      "Iteration 1550: theta=[[3.49396204]\n",
      " [7.52958882]\n",
      " [5.27220166]], error=[[ 0.20202007]\n",
      " [-0.4194142 ]\n",
      " [ 0.30701246]]\n",
      "Iteration 1600: theta=[[3.46506072]\n",
      " [7.58942622]\n",
      " [5.27529885]], error=[[ 0.19847481]\n",
      " [-0.40266075]\n",
      " [ 0.28793003]]\n",
      "Iteration 1650: theta=[[3.43804867]\n",
      " [7.64667352]\n",
      " [5.27723391]], error=[[ 0.19474386]\n",
      " [-0.38660968]\n",
      " [ 0.27015149]]\n",
      "Iteration 1700: theta=[[3.41279239]\n",
      " [7.7014541 ]\n",
      " [5.27813228]], error=[[ 0.1908592 ]\n",
      " [-0.37122909]\n",
      " [ 0.25358214]]\n",
      "Iteration 1750: theta=[[3.38916826]\n",
      " [7.75388466]\n",
      " [5.27810832]], error=[[ 0.18684964]\n",
      " [-0.35648871]\n",
      " [ 0.23813442]]\n",
      "Iteration 1800: theta=[[3.3670618 ]\n",
      " [7.80407568]\n",
      " [5.2772662 ]], error=[[ 0.18274111]\n",
      " [-0.34235979]\n",
      " [ 0.22372732]]\n",
      "Iteration 1850: theta=[[3.34636696]\n",
      " [7.85213179]\n",
      " [5.27570074]], error=[[ 0.17855691]\n",
      " [-0.32881502]\n",
      " [ 0.21028593]]\n",
      "Iteration 1900: theta=[[3.32698551]\n",
      " [7.8981521 ]\n",
      " [5.27349821]], error=[[ 0.17431793]\n",
      " [-0.31582845]\n",
      " [ 0.19774092]]\n",
      "Iteration 1950: theta=[[3.30882641]\n",
      " [7.94223055]\n",
      " [5.27073701]], error=[[ 0.17004286]\n",
      " [-0.30337541]\n",
      " [ 0.18602815]]\n",
      "Iteration 2000: theta=[[3.29180531]\n",
      " [7.9844562 ]\n",
      " [5.26748835]], error=[[ 0.16574841]\n",
      " [-0.29143244]\n",
      " [ 0.17508826]]\n",
      "Iteration 2050: theta=[[3.275844  ]\n",
      " [8.02491353]\n",
      " [5.26381682]], error=[[ 0.16144942]\n",
      " [-0.27997722]\n",
      " [ 0.16486631]]\n",
      "Iteration 2100: theta=[[3.26086998]\n",
      " [8.06368272]\n",
      " [5.259781  ]], error=[[ 0.1571591 ]\n",
      " [-0.2689885 ]\n",
      " [ 0.15531144]]\n",
      "Iteration 2150: theta=[[3.24681601]\n",
      " [8.10083986]\n",
      " [5.25543391]], error=[[ 0.15288914]\n",
      " [-0.25844609]\n",
      " [ 0.14637656]]\n",
      "Iteration 2200: theta=[[3.23361975]\n",
      " [8.13645723]\n",
      " [5.25082351]], error=[[ 0.14864984]\n",
      " [-0.24833071]\n",
      " [ 0.13801807]]\n",
      "Iteration 2250: theta=[[3.22122333]\n",
      " [8.17060348]\n",
      " [5.24599313]], error=[[ 0.14445025]\n",
      " [-0.23862403]\n",
      " [ 0.13019559]]\n",
      "Iteration 2300: theta=[[3.20957307]\n",
      " [8.20334386]\n",
      " [5.24098186]], error=[[ 0.14029828]\n",
      " [-0.22930857]\n",
      " [ 0.12287171]]\n",
      "Iteration 2350: theta=[[3.19861913]\n",
      " [8.23474041]\n",
      " [5.23582491]], error=[[ 0.13620083]\n",
      " [-0.22036767]\n",
      " [ 0.11601177]]\n",
      "Iteration 2400: theta=[[3.18831527]\n",
      " [8.26485214]\n",
      " [5.23055393]], error=[[ 0.13216382]\n",
      " [-0.21178544]\n",
      " [ 0.10958365]]\n",
      "Iteration 2450: theta=[[3.17861852]\n",
      " [8.29373517]\n",
      " [5.22519735]], error=[[ 0.12819236]\n",
      " [-0.20354675]\n",
      " [ 0.1035576 ]]\n",
      "Iteration 2500: theta=[[3.16948899]\n",
      " [8.32144293]\n",
      " [5.21978066]], error=[[ 0.12429076]\n",
      " [-0.19563712]\n",
      " [ 0.09790601]]\n",
      "Iteration 2550: theta=[[3.16088959]\n",
      " [8.34802629]\n",
      " [5.21432661]], error=[[ 0.12046265]\n",
      " [-0.18804277]\n",
      " [ 0.09260332]]\n",
      "Iteration 2600: theta=[[3.15278588]\n",
      " [8.37353369]\n",
      " [5.20885552]], error=[[ 0.11671105]\n",
      " [-0.18075053]\n",
      " [ 0.08762578]]\n",
      "Iteration 2650: theta=[[3.14514584]\n",
      " [8.39801127]\n",
      " [5.20338547]], error=[[ 0.11303839]\n",
      " [-0.17374783]\n",
      " [ 0.08295139]]\n",
      "Iteration 2700: theta=[[3.13793967]\n",
      " [8.42150302]\n",
      " [5.19793249]], error=[[ 0.1094466 ]\n",
      " [-0.16702264]\n",
      " [ 0.07855974]]\n",
      "Iteration 2750: theta=[[3.13113969]\n",
      " [8.44405088]\n",
      " [5.19251074]], error=[[ 0.10593715]\n",
      " [-0.16056352]\n",
      " [ 0.07443188]]\n",
      "Iteration 2800: theta=[[3.12472011]\n",
      " [8.46569481]\n",
      " [5.18713269]], error=[[ 0.10251111]\n",
      " [-0.15435948]\n",
      " [ 0.07055022]]\n",
      "Iteration 2850: theta=[[3.11865696]\n",
      " [8.48647295]\n",
      " [5.1818093 ]], error=[[ 0.09916918]\n",
      " [-0.14840005]\n",
      " [ 0.06689842]]\n",
      "Iteration 2900: theta=[[3.11292789]\n",
      " [8.5064217 ]\n",
      " [5.1765501 ]], error=[[ 0.09591173]\n",
      " [-0.14267522]\n",
      " [ 0.06346132]]\n",
      "Iteration 2950: theta=[[3.10751212]\n",
      " [8.52557578]\n",
      " [5.17136338]], error=[[ 0.09273883]\n",
      " [-0.13717539]\n",
      " [ 0.06022484]]\n",
      "Iteration 3000: theta=[[3.10239026]\n",
      " [8.54396835]\n",
      " [5.16625626]], error=[[ 0.08965032]\n",
      " [-0.13189142]\n",
      " [ 0.05717589]]\n",
      "Iteration 3050: theta=[[3.09754427]\n",
      " [8.56163107]\n",
      " [5.16123483]], error=[[ 0.08664578]\n",
      " [-0.12681453]\n",
      " [ 0.0543023 ]]\n",
      "Iteration 3100: theta=[[3.09295731]\n",
      " [8.57859418]\n",
      " [5.15630425]], error=[[ 0.0837246 ]\n",
      " [-0.12193634]\n",
      " [ 0.05159275]]\n",
      "Iteration 3150: theta=[[3.08861368]\n",
      " [8.59488656]\n",
      " [5.1514688 ]], error=[[ 0.080886  ]\n",
      " [-0.11724882]\n",
      " [ 0.04903674]]\n",
      "Iteration 3200: theta=[[3.08449874]\n",
      " [8.61053585]\n",
      " [5.14673203]], error=[[ 0.07812904]\n",
      " [-0.11274428]\n",
      " [ 0.04662447]]\n",
      "Iteration 3250: theta=[[3.08059883]\n",
      " [8.62556842]\n",
      " [5.14209679]], error=[[ 0.07545263]\n",
      " [-0.10841537]\n",
      " [ 0.04434685]]\n",
      "Iteration 3300: theta=[[3.07690119]\n",
      " [8.64000951]\n",
      " [5.13756528]], error=[[ 0.07285559]\n",
      " [-0.10425504]\n",
      " [ 0.04219539]]\n",
      "Iteration 3350: theta=[[3.0733939 ]\n",
      " [8.65388326]\n",
      " [5.13313919]], error=[[ 0.07033662]\n",
      " [-0.10025655]\n",
      " [ 0.04016222]]\n",
      "Iteration 3400: theta=[[3.07006583]\n",
      " [8.66721275]\n",
      " [5.12881967]], error=[[ 0.06789436]\n",
      " [-0.09641344]\n",
      " [ 0.03823998]]\n",
      "Iteration 3450: theta=[[3.06690657]\n",
      " [8.68002006]\n",
      " [5.12460745]], error=[[ 0.06552734]\n",
      " [-0.0927195 ]\n",
      " [ 0.03642182]]\n",
      "Iteration 3500: theta=[[3.06390641]\n",
      " [8.69232633]\n",
      " [5.12050285]], error=[[ 0.06323406]\n",
      " [-0.0891688 ]\n",
      " [ 0.03470136]]\n",
      "Iteration 3550: theta=[[3.06105623]\n",
      " [8.70415178]\n",
      " [5.11650584]], error=[[ 0.06101297]\n",
      " [-0.08575567]\n",
      " [ 0.03307266]]\n",
      "Iteration 3600: theta=[[3.05834752]\n",
      " [8.71551578]\n",
      " [5.11261608]], error=[[ 0.05886248]\n",
      " [-0.08247464]\n",
      " [ 0.03153017]]\n",
      "Iteration 3650: theta=[[3.05577231]\n",
      " [8.72643687]\n",
      " [5.10883293]], error=[[ 0.05678097]\n",
      " [-0.07932049]\n",
      " [ 0.03006871]]\n",
      "Iteration 3700: theta=[[3.05332314]\n",
      " [8.7369328 ]\n",
      " [5.10515553]], error=[[ 0.05476679]\n",
      " [-0.07628821]\n",
      " [ 0.02868344]]\n",
      "Iteration 3750: theta=[[3.050993  ]\n",
      " [8.74702057]\n",
      " [5.10158281]], error=[[ 0.05281829]\n",
      " [-0.073373  ]\n",
      " [ 0.02736986]]\n",
      "Iteration 3800: theta=[[3.04877534]\n",
      " [8.75671649]\n",
      " [5.09811349]], error=[[ 0.05093381]\n",
      " [-0.07057024]\n",
      " [ 0.02612375]]\n",
      "Iteration 3850: theta=[[3.04666401]\n",
      " [8.76603615]\n",
      " [5.09474615]], error=[[ 0.04911168]\n",
      " [-0.06787552]\n",
      " [ 0.02494117]]\n",
      "Iteration 3900: theta=[[3.04465325]\n",
      " [8.77499452]\n",
      " [5.0914792 ]], error=[[ 0.04735025]\n",
      " [-0.0652846 ]\n",
      " [ 0.02381845]]\n",
      "Iteration 3950: theta=[[3.04273763]\n",
      " [8.78360592]\n",
      " [5.08831097]], error=[[ 0.04564787]\n",
      " [-0.06279341]\n",
      " [ 0.02275215]]\n",
      "Iteration 4000: theta=[[3.04091207]\n",
      " [8.79188412]\n",
      " [5.08523966]], error=[[ 0.04400289]\n",
      " [-0.06039805]\n",
      " [ 0.02173903]]\n",
      "Iteration 4050: theta=[[3.0391718 ]\n",
      " [8.79984226]\n",
      " [5.0822634 ]], error=[[ 0.04241369]\n",
      " [-0.05809477]\n",
      " [ 0.0207761 ]]\n",
      "Iteration 4100: theta=[[3.03751232]\n",
      " [8.807493  ]\n",
      " [5.07938025]], error=[[ 0.04087867]\n",
      " [-0.05587997]\n",
      " [ 0.01986053]]\n",
      "Iteration 4150: theta=[[3.03592942]\n",
      " [8.81484845]\n",
      " [5.07658821]], error=[[ 0.03939624]\n",
      " [-0.05375021]\n",
      " [ 0.01898967]]\n",
      "Iteration 4200: theta=[[3.03441912]\n",
      " [8.82192021]\n",
      " [5.07388523]], error=[[ 0.03796484]\n",
      " [-0.05170218]\n",
      " [ 0.01816105]]\n",
      "Iteration 4250: theta=[[3.0329777 ]\n",
      " [8.82871944]\n",
      " [5.07126924]], error=[[ 0.03658294]\n",
      " [-0.0497327 ]\n",
      " [ 0.01737235]]\n",
      "Iteration 4300: theta=[[3.03160162]\n",
      " [8.83525683]\n",
      " [5.06873815]], error=[[ 0.03524903]\n",
      " [-0.04783871]\n",
      " [ 0.01662139]]\n",
      "Iteration 4350: theta=[[3.03028757]\n",
      " [8.84154263]\n",
      " [5.06628984]], error=[[ 0.03396164]\n",
      " [-0.04601728]\n",
      " [ 0.01590612]]\n",
      "Iteration 4400: theta=[[3.02903244]\n",
      " [8.84758669]\n",
      " [5.0639222 ]], error=[[ 0.0327193 ]\n",
      " [-0.04426561]\n",
      " [ 0.01522464]]\n",
      "Iteration 4450: theta=[[3.02783328]\n",
      " [8.85339845]\n",
      " [5.06163311]], error=[[ 0.0315206 ]\n",
      " [-0.04258098]\n",
      " [ 0.01457513]]\n",
      "Iteration 4500: theta=[[3.02668731]\n",
      " [8.85898698]\n",
      " [5.05942046]], error=[[ 0.03036416]\n",
      " [-0.0409608 ]\n",
      " [ 0.01395592]]\n",
      "Iteration 4550: theta=[[3.02559191]\n",
      " [8.86436097]\n",
      " [5.05728216]], error=[[ 0.02924862]\n",
      " [-0.03940259]\n",
      " [ 0.01336539]]\n",
      "Iteration 4600: theta=[[3.0245446 ]\n",
      " [8.86952878]\n",
      " [5.05521612]], error=[[ 0.02817265]\n",
      " [-0.03790394]\n",
      " [ 0.01280207]]\n",
      "Iteration 4650: theta=[[3.02354305]\n",
      " [8.87449842]\n",
      " [5.05322029]], error=[[ 0.02713497]\n",
      " [-0.03646256]\n",
      " [ 0.01226454]]\n",
      "Iteration 4700: theta=[[3.02258505]\n",
      " [8.8792776 ]\n",
      " [5.05129263]], error=[[ 0.02613431]\n",
      " [-0.03507624]\n",
      " [ 0.01175148]]\n",
      "Iteration 4750: theta=[[3.02166852]\n",
      " [8.8838737 ]\n",
      " [5.04943114]], error=[[ 0.02516945]\n",
      " [-0.03374286]\n",
      " [ 0.01126164]]\n",
      "Iteration 4800: theta=[[3.02079147]\n",
      " [8.88829382]\n",
      " [5.04763385]], error=[[ 0.02423921]\n",
      " [-0.03246037]\n",
      " [ 0.01079384]]\n",
      "Iteration 4850: theta=[[3.01995204]\n",
      " [8.89254477]\n",
      " [5.04589882]], error=[[ 0.02334241]\n",
      " [-0.03122682]\n",
      " [ 0.01034699]]\n",
      "Iteration 4900: theta=[[3.01914846]\n",
      " [8.89663311]\n",
      " [5.04422413]], error=[[ 0.02247793]\n",
      " [-0.03004033]\n",
      " [ 0.00992002]]\n",
      "Iteration 4950: theta=[[3.01837906]\n",
      " [8.90056511]\n",
      " [5.04260792]], error=[[ 0.02164467]\n",
      " [-0.02889908]\n",
      " [ 0.00951197]]\n",
      "Theta computed using batch gradient descent: [[3.01765668]\n",
      " [8.90427262]\n",
      " [5.04107902]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "#Suppose we have 3 sample and 2 features\n",
    "features = np.random.rand(3,2)\n",
    "#add ones for bias\n",
    "features_with_bias = np.c_[np.ones((3,1)),features] #add x0=1 for each instance\n",
    "# make weights a column vector so y_actual is (3,1)\n",
    "weights_actual = np.array([3, 9, 5]).reshape(-1, 1)  # bias 3, theta_1=9, theta_2=5\n",
    "# compute y_actual as a (3,1) column vector\n",
    "y_actual = features_with_bias.dot(weights_actual)\n",
    "print(\"Feature-matrix:\", features_with_bias)\n",
    "print(\"y_Actual:\", y_actual)\n",
    "theta_computed = compute_weights(features_with_bias, y_actual)\n",
    "print(\"Theta computed using batch gradient descent:\", theta_computed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da480ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_cloudxlabs (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
